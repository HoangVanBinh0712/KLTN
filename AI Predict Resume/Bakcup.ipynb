{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Delete final_data.csv before run\n","import numpy as np \n","import pandas as pd\n","data_raw = pd.read_csv('./DataSet.csv',delimiter=',', encoding = 'ISO-8859-1')\n","data_raw.head()\n","#Drop Original Resume\n","data = data_raw.drop('Resume', axis=1)\n","data.head()\n","#Đa dạng hóa dữ liệu\n","# Một item ở 1 category sẽ generate ra thêm 2 samples với cùng resume nhưng khác skill\n","# Skill sẽ được tổng hợp bởi những sample cùng loại \n","# Sau đó nối ngẫu nhiên skill của n samples khác lại\n","from pandas import DataFrame\n","import random\n","labels = np.unique(data['Category'])\n","res = []\n","for i in labels:\n","    x = data[data['Category'] == i].values\n","    print(i, x.shape)\n","    # Tron data\n","    len = x.shape\n","    for j in range(0, len[0]):\n","        #tron n lan\n","        num = 2\n","        for c in range (0, num):\n","            rands = random.sample(range(0, len[0]), num)\n","            #[1,4,9]\n","            newSamples = x[j]\n","            d2 = ''\n","            for x1 in range(0, num):\n","                d2 += str(x[rands[x1]][2])\n","            newSamples[2] = d2\n","            x = np.vstack([x, newSamples])\n","\n","    if res.__len__() == 0:\n","        res = x\n","    else:\n","        res = np.vstack([res, x])\n","print(res.shape)\n","df = DataFrame(\n","    {'Category': res[:, 0], 'Resume': res[:, 1], 'Skill': res[:, 2]})\n","df.to_csv('final_data.csv',  index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":973},"executionInfo":{"elapsed":797,"status":"ok","timestamp":1677078059163,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"V643wKs5dNA1","outputId":"db995276-f1fd-4c99-c953-4b05909fb018"},"outputs":[],"source":["#%% Import libraries\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score\n","from sklearn.utils import shuffle\n","import string\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import warnings\n","import seaborn as sns\n","warnings.filterwarnings('ignore')\n","np.set_printoptions(precision=4)\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","#%% Load data\n","data = pd.read_csv('./final_data.csv', delimiter=',', encoding = 'ISO-8859-1')\n","data.head()\n","#%%Bar graph visualization\n","plt.figure(figsize=(15,15))\n","plt.xticks(rotation=90)\n","sns.countplot(y=\"Category\", data=data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12199,"status":"ok","timestamp":1677078088110,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"ZDSMWQhQd1wV","outputId":"3879a43f-7f3d-4ceb-caf0-fab2fc2c2583"},"outputs":[],"source":["# %% Get set of stopwords\n","#stopwords_set = set(stopwords.words('english')+['``',\"''\"])\n","\n","gist_file = open(\"./stopword.txt\", \"r\")\n","try:\n","    content = gist_file.read()\n","    stopwords_set = content.split(\",\")\n","finally:\n","    gist_file.close()\n","stopwords_set = set(stopwords_set)\n","\n","# %% Function to clean resume text\n","def clean_text(resume_text):\n","    try:\n","      resume_text = re.sub('http\\S+\\s*', ' ', resume_text) \n","      resume_text = re.sub('RT|cc', ' ', resume_text) \n","      resume_text = re.sub('#\\S+', '', resume_text) \n","      resume_text = re.sub('@\\S+', '  ', resume_text) \n","      resume_text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resume_text) \n","      resume_text = re.sub(r'[^\\x00-\\x7f]',r' ', resume_text) \n","      resume_text = re.sub('\\s+', ' ', resume_text) \n","      resume_text = resume_text.lower()  \n","      resume_text_tokens = word_tokenize(resume_text) \n","      filtered_text = [w for w in resume_text_tokens if not w in stopwords_set] \n","      return ' '.join(filtered_text)\n","    except:\n","      return ''\n","    \n","\n","# %% Print a sample original resume\n","print('--- Original resume ---')\n","#print(data['Resume'][0])\n","data['cleaned_resume'] = data.Resume.apply(lambda x: clean_text(x))\n","data['cleaned_skill'] = data.Skill.apply(lambda x: clean_text(x))\n","\n","print('--- Cleaned resume ---')\n","#print(data['cleaned_resume'][0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1677078090962,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"ImhmuW1HXYcg","outputId":"1b38b799-d55d-416b-8d31-77cc2f02acf8"},"outputs":[],"source":["print(data['cleaned_skill'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677078096040,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"rCv2TuJKfneJ","outputId":"27c9fb99-5ac7-487d-e179-0824f978d402"},"outputs":[],"source":["\n","# %%Get features and labels from data and shuffle\n","features1 = data['cleaned_resume'].values\n","features2 = data['cleaned_skill'].values\n","\n","original_labels = data['Category'].values\n","labels = original_labels[:]\n","data_size = data.__len__()\n","for i in range(data_size):\n","  labels[i] = str(str(labels[i]).lower())\n","  labels[i] = labels[i].replace(\" \", \"\") \n","  labels[i] = labels[i].replace(\"-\", \"\") \n","\n","features1,features2, labels = shuffle(features1,features2, labels)\n","\n","\n","# %% Split for train and test\n","train_split = 0.75\n","train_size = int(train_split * data_size)\n","\n","train_features = [features1[:train_size],features2[:train_size]]\n","train_labels = labels[:train_size]\n","\n","test_features = [features1[train_size:],features2[train_size:]]\n","test_labels = labels[train_size:]\n","\n","# Print size of each split\n","print(train_labels.__len__())\n","print(test_labels.__len__())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2054,"status":"ok","timestamp":1677078103805,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"Q8RBBcbIiHl8"},"outputs":[],"source":["\n","# %%Tokenize feature data and print word dictionary\n","vocab_size = 6000\n","oov_tok = '<OOV>'\n","feature_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n","feature_tokenizer.fit_on_texts(features1)\n","feature_tokenizer.fit_on_texts(features2)\n","feature_index = feature_tokenizer.word_index\n","# Print example sequences from train and test datasets\n","train_feature_sequences = [feature_tokenizer.texts_to_sequences(train_features[0]),feature_tokenizer.texts_to_sequences(train_features[1])]\n","test_feature_sequences = [feature_tokenizer.texts_to_sequences(test_features[0]),feature_tokenizer.texts_to_sequences(test_features[1])]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396,"status":"ok","timestamp":1677078107611,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"X_XZjjKYhmKu","outputId":"adff6050-1f30-47e5-e90a-f328522ccf76"},"outputs":[],"source":["\n","# %% Tokenize label data and print label dictionary\n","label_tokenizer = Tokenizer(lower=True)\n","label_tokenizer.fit_on_texts(labels)\n","label_index = label_tokenizer.word_index\n","print(dict(list(label_index.items())))\n","# Print example label encodings from train and test datasets\n","train_label_sequences = label_tokenizer.texts_to_sequences(train_labels)\n","test_label_sequences = label_tokenizer.texts_to_sequences(test_labels)\n","\n","\n","# %%Pad sequences for feature data\n","max_length = 1200\n","trunc_type = 'post'\n","pad_type = 'post'\n","\n","train_feature_padded = np.array([pad_sequences(train_feature_sequences[0], maxlen=max_length, padding=pad_type, truncating=trunc_type),pad_sequences(train_feature_sequences[1], maxlen=max_length, padding=pad_type, truncating=trunc_type)])\n","test_feature_padded = np.array([pad_sequences(test_feature_sequences[0], maxlen=max_length, padding=pad_type, truncating=trunc_type),pad_sequences(test_feature_sequences[1], maxlen=max_length, padding=pad_type, truncating=trunc_type)])\n","\n","embedding_dim = 64\n","units = np.unique(data['Category']).__len__() + 1\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Embedding, Concatenate, Flatten, Dense\n","\n","# First input\n","input1_weight = 0.6\n","input2_weight = 0.4\n","input1 = Input(shape=(max_length,), name='input1')\n","x1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input1)*input1_weight\n","\n","# Second input\n","input2 = Input(shape=(max_length,), name='input2')\n","x2 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input2)*input2_weight\n","\n","# Concatenate embeddings\n","x = Concatenate()([Flatten()(x1), Flatten()(x2)])\n","\n","# Add more layers to the model\n","x = Dense(64, activation='relu')(x)\n","x = Dense(32, activation='relu')(x)\n","output = Dense(units, activation='softmax')(x)\n","\n","# Create the model\n","model = tf.keras.Model(inputs=[input1, input2], outputs=output)\n","\n","#%% # Compile the model and convert train/test data into NumPy arrays\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# train_feature_padded = np.array(train_feature_padded)\n","# test_feature_padded = np.array(test_feature_padded)\n","train_label_sequences = np.array(train_label_sequences)\n","test_label_sequences = np.array(test_label_sequences)\n","\n","# %%\n","num_epochs = 100\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":563574,"status":"ok","timestamp":1677078680783,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"P1Qmf5uOf38x","outputId":"51d47075-e7ae-452d-9c0b-cc0fd9022c67"},"outputs":[],"source":["history = model.fit((train_feature_padded[0],train_feature_padded[1]), train_label_sequences, epochs=num_epochs, validation_data=((test_feature_padded[0],test_feature_padded[1]), test_label_sequences), verbose=2)\n","score = model.evaluate((test_feature_padded[0],test_feature_padded[1]), test_label_sequences, verbose=1)\n","print(\"Test Accuracy:\", score[1])\n","\n","#%% Draw accuracy model & loss model\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train','test'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train','test'], loc='upper left')\n","plt.show()\n","\n","# %%\n","model.save('models_P')\n","\n","# %% Save feature tokenizer\n","with open('feature_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(feature_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# %% Save reverse dictionary of labels to encodings\n","label_to_encoding = dict(list(label_index.items()))\n","\n","encoding_to_label = {}\n","for k, v in label_to_encoding.items():\n","  encoding_to_label[v] = k\n","print(encoding_to_label)\n","with open('dictionary.pickle', 'wb') as handle:\n","    pickle.dump(encoding_to_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1677078800602,"user":{"displayName":"Binh Hoàng Văn","userId":"13469692746222581918"},"user_tz":-420},"id":"S5DstK9_kazh","outputId":"b523aa96-a8a3-404b-f80e-409bafb70df5"},"outputs":[],"source":["# %% Create padded sequence for example\n","input = \"\"\"Work History      August 2010  -  Current    Teacher   |   Company Name   |   City  ,   State     Executed lesson plans and evaluated the effectiveness through assessment and reflection Created visual supports, using technology to enhance and to differentiate instruction Use the Go Math, Eureka, & Engage NY, IXL & Khan Academy resources to provide math instruction Lead inquiry-based student investigations for the FOSS Science Program using Next Generation Standards Work with the Common Core & Next Generation Standards to provide structured instruction Provide Smarter Balanced NWEA Test Preparation for Mathematics & Science Create instructional materials and strategies consistent with student learning and behavioral needs Utilize the Positive Behavior Support (PBS) Program to provide positive reinforcement as a preventive measure for disruptive and/or inappropriate behavior Collaborate with grade partners to provide consistent instruction, and to develop and implement grade-level goals Update a teacher infinite campus with homework, grades and events to extend parental and student communication.         August 2008  -  June 2010    6th Grade Teacher   |   Company Name   |   City  ,   State     Provide a balanced literacy program to fifth graders based on Shelton's curriculum goal of the whole, small, whole Reader's Workshop Model, in order to increase student growth in reading stamina and comprehension skills Use various strategies and procedures for small group instruction to increase student growth in vocabulary, fluency, and comprehension Utilize Technology across multiple content areas Teach students strategies for previewing books and choose a book that is \"just-right\" Provide Writing Instruction through the Writer's Workshop model Utilize the Houghton Mifflin Anthology to model various meta-cognitive reading strategies Supplement the curriculum with various non-fiction texts Utilize Literacy Work Stations during small group instruction time, such as an Independent Daily Reading Station, Word Study Station, and a CMT Strand Work Station Provide small group instruction based on analysis of weak CMT strands, as well as skill needs in the subjects of Reading, Writing, and Math Administer and analyze the Developmental Reading Assessment (DRA 2) Provide application lessons to improve students' Degrees of Reading Power skills on the Connecticut Mastery Tests Used the Everyday Mathematics Curriculum to provide math instruction Lead inquiry-based student investigations for the FOSS Science Program Provide Connecticut Mastery Test Preparation for Reading, Writing, Mathematics, and Science Create instructional materials and strategies consistent with student learning and behavioral needs Utilize the Positive Behavior Support (PBS) Program to provide positive reinforcement as a preventive measure for disruptive and/or inappropriate behavior.         September 2007  -  June 2008    Special Education Tutor   |   Company Name   |   City  ,   State     Modify math curriculum and created worksheets for a sixth grade student in accordance with IEP objectives Create accommodated quizzes, tests, lesson and study guides for sixth grade Social Studies curriculum Provide small group instruction for core content subjects in K-6 general education classrooms Provide assistance to general education teacher using the Inclusion Model for instruction Assess Kindergarten students using Shelton Public Schools Assessment Administer the Brigance Inventory of Basic Skills to special education students Administer CMT's to special education students Provide accommodations during sixth grade Writer's Workshop for a group of six students Planned and implemented an introduction to the 6th grade Science Embedded Task utilizing the Scientific Method Create visual supports using technology to enhance and to differentiate instruction Observe and assist with Everyday Mathematics, Wilson Reading, Houghton Mifflin, Scott Foresman Social Studies lesson, CMT Prep, and Literature Circles.         Education     2003   Diploma  :       Derby High School  ,   City  ,   State         2007   NCATE Endorsement, National Council for Accreditation of Teacher Education Connecticut Collaboration (Elementary Regular Education/Special Education) (013, 165 certifications)  :       Southern Connecticut State University  ,   City  ,   State         2010   Collaborative Endorsement (Elementary Regular/Special Education) (013, 165)  :       Southern Connecticut State University  ,   City  ,   State   NCATE Endorsement, National Council for Accreditation of Teacher Education Remedial Reading and Remedial Language Arts, Grades K-12 (102 Certification) GPA: 3.85 Magna Cum Laude, Outstanding Special Education Senior Award      May 2007   Bachelor of Arts  :   Psychology   Psychology         Passed Praxis II (Regular Education) *Passed Praxis II (Special Education) *Remedial Reading and Remedial Language Arts, Grades K-12 (102 Certification) *Passed Reading Test  :             Skills    Arts, Basic, book, content, Council, in K, Inspiration, instruction, Internet Browsers, Inventory, lesson plans, materials, Math, Mathematics, Excel, Microsoft Office, PowerPoint, Publisher, Microsoft Windows XP Professional, Word, Next, non-fiction, Speaking, Reading, Scientific, structured, Teacher, Writer      Additional Information      MEMBERSHIPS/HONORS: Academic Dean's List at Southern Connecticut State University  - (Fall 2003, Fall 2004, Fall 2005, Spring 2006, Fall 2006) Psi-Chi psychology national honor society Golden Key national honor society Kappa Delta Pi national honor society Most Outstanding Special Education Senior\"\"\"\n","resume_example = clean_text(input)\n","example_sequence = feature_tokenizer.texts_to_sequences([resume_example])\n","example_padded = pad_sequences(example_sequence, maxlen=max_length, padding=pad_type, truncating=trunc_type)\n","example_padded = np.array(example_padded)\n","\n","input2 = \"\"\"TEACHER         Professional Summary    An experienced human resource training professional with demonstrated success in developing, delivering and evaluating, corporate training programs, 2+ years of work with newly developed tools for rapid e-learning development. Special skills in online training for a variety of audiences. Recognized for alignment of training solutions with business goals, management of project and people, process improvement, needs analysis and training evaluation.      Core Qualifications          Microsoft Words  Outlook  Internet  PowerPoint  ADDIE MODEL  Microsoft Office  Adobe Photoshop  Audacity Sound Booth  Moodle  AdobeCS5  Captivate  Adobe Premier  Flash  Blackboard 9.1  PeopleSoft\"\"\"\n","resume_example2 = clean_text(input2)\n","example_sequence2 = feature_tokenizer.texts_to_sequences([resume_example2])\n","example_padded2 = pad_sequences(example_sequence2, maxlen=max_length, padding=pad_type, truncating=trunc_type)\n","example_padded2 = np.array(example_padded2)\n","#print(example_padded)\n","# Make a prediction\n","print((example_padded,example_padded2))\n","prediction = model.predict((example_padded,example_padded2))\n","# Verify that prediction has correct format\n","# print(prediction[0])\n","# print(np.sum(prediction[0])) \n","\n","# Indices of top 5 most probable solutions\n","indices = np.argpartition(prediction[0], -5)[-5:]\n","indices = indices[np.argsort(prediction[0][indices])]\n","indices = list(reversed(indices))\n","print(indices)\n","\n","# Find maximum value in prediction and its index\n","#print(prediction[0])\n","# print(max(prediction[0])) \n","# print(np.argmax(prediction[0])) \n","\n","# \n","for index in indices:\n","    print(prediction[0][index]*100,\"% \",encoding_to_label[index])\n","\n","# %%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPeSyhl11hTPW0N/pmk5doV","mount_file_id":"1lBIQ78cQ-E0luirOHym1gq37whbBPZOS","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"vscode":{"interpreter":{"hash":"df92f1e62a552003a5454108dd53327b624a24cb0b968cd3a92df9f90f04b441"}}},"nbformat":4,"nbformat_minor":0}
